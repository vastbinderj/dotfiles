
.TH "GCLOUD_DATAPROC_JOBS_SUBMIT_PYSPARK" 1



.SH "NAME"
.HP
gcloud dataproc jobs submit pyspark \- submit a PySpark job to a cluster



.SH "SYNOPSIS"
.HP
\f5gcloud dataproc jobs submit pyspark\fR \fIPY_FILE\fR \fB\-\-cluster\fR=\fICLUSTER\fR [\fB\-\-archives\fR=[\fIARCHIVE\fR,...]] [\fB\-\-async\fR] [\fB\-\-bucket\fR=\fIBUCKET\fR] [\fB\-\-driver\-log\-levels\fR=[\fIPACKAGE\fR=\fILEVEL\fR,...]] [\fB\-\-files\fR=[\fIFILE\fR,...]] [\fB\-\-jars\fR=[\fIJAR\fR,...]] [\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-properties\fR=[\fIPROPERTY\fR=\fIVALUE\fR,...]] [\fB\-\-py\-files\fR=[\fIPY_FILE\fR,...]] [\fIGLOBAL\-FLAG\ ...\fR] [\-\-\ \fIJOB_ARGS\fR\ ...]



.SH "DESCRIPTION"

Submit a PySpark job to a cluster.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fIPY_FILE\fR
The main .py file to run as the driver.

.TP 2m
[\-\- \fIJOB_ARGS\fR ...]
The arguments to pass to the driver.

The '\-\-' argument must be specified between gcloud specific args on the left
and JOB_ARGS on the right.


.RE
.sp

.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m
\fB\-\-cluster\fR=\fICLUSTER\fR
The Dataproc cluster to submit the job to.


.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-archives\fR=[\fIARCHIVE\fR,...]
Comma separated list of archives to be provided to the job. must be one of the
following file formats: .zip, .tar, .tar.gz, or .tgz.

.TP 2m
\fB\-\-async\fR
Does not wait for the job to run.

.TP 2m
\fB\-\-bucket\fR=\fIBUCKET\fR
The Cloud Storage bucket to stage files in. Defaults to the cluster's configured
bucket.

.TP 2m
\fB\-\-driver\-log\-levels\fR=[\fIPACKAGE\fR=\fILEVEL\fR,...]
A list of package to log4j log level pairs to configure driver logging. For
example: root=FATAL,com.example=INFO

.TP 2m
\fB\-\-files\fR=[\fIFILE\fR,...]
Comma separated list of files to be provided to the job.

.TP 2m
\fB\-\-jars\fR=[\fIJAR\fR,...]
Comma separated list of jar files to be provided to the executor and driver
classpaths.

.TP 2m
\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]
A list of label KEY=VALUE pairs to add.

.TP 2m
\fB\-\-properties\fR=[\fIPROPERTY\fR=\fIVALUE\fR,...]
A list of key value pairs to configure PySpark.

.TP 2m
\fB\-\-py\-files\fR=[\fIPY_FILE\fR,...]
Comma separated list of Python files to be provided to the job.Must be one of
the following file formats" .py, ,.zip, or .egg


.RE
.sp

.SH "GLOBAL FLAGS"

Run \fB$ gcloud help\fR for a description of flags available to all commands.



.SH "EXAMPLES"

To submit a PySpark job with a local script, run:

.RS 2m
$ gcloud dataproc jobs submit pyspark \-\-cluster my_cluster \e
    my_script.py
.RE

To submit a Spark job that runs a script that is already on the cluster, run:

.RS 2m
$ gcloud dataproc jobs submit pyspark \-\-cluster my_cluster \e
    file:///usr/lib/spark/examples/src/main/python/pi.py 100
.RE
